#@include file-fluent.conf

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    # Include files 
    @include 01_sources.conf
    @include 02_filters.conf
    @include 04_outputs.conf

  01_sources.conf: |-
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

  02_filters.conf: |-
    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>

    # Concatenate multi-line logs
    <filter **>
      @id filter_concat
      @type concat
      key message
      multiline_end_regexp /\n$/
      separator ""
      timeout_label @NORMAL
      flush_interval 5
    </filter>

    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id filter_kubernetes_metadata
      @type kubernetes_metadata
    </filter>

    # Fixes json fields in Elasticsearch
    <filter kubernetes.**>
      @id filter_parser
      @type parser
      key_name log
      reserve_time true
      reserve_data true
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

  03_dispatch.conf: |-

  04_outputs.conf: |-
    # handle timeout log lines from concat plugin
    <match **>
      @type relabel
      @label @NORMAL
    </match>

    <label @NORMAL>
    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch.logging'}"
      port 9200
      path ""
      scheme http
      type_name _doc
      logstash_format true
      logstash_prefix logstash
      reconnect_on_error true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>
    </label>


#@include file-fluent.conf

# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: fluentd-config
# data:
#   fluent.conf: |-
#     ################################################################
#     # This source gets all logs from local docker host
#     @include pods-kind-fluent.conf
#     #@include pods-fluent.conf
#     #@include file-fluent.conf
#     @include elastic-fluent.conf
#   pods-kind-fluent.conf: |-
#     <source>
#       @type tail
#       read_from_head true
#       tag kubernetes.*
#       path /var/log/containers/*.log
#       pos_file /var/log/fluentd-containers.log.pos
#       exclude_path ["/var/log/containers/fluent*"]
#       <parse>
#         @type regexp
#         #https://regex101.com/r/ZkOBTI/1
#         expression ^(?<time>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.[^Z]*Z)\s(?<stream>[^\s]+)\s(?<character>[^\s])\s(?<message>.*)$
#         #time_format %Y-%m-%dT%H:%M:%S.%NZ
#       </parse>
#     </source>

#     <filter kubernetes.**>
#       @type kubernetes_metadata
#       @id filter_kube_metadata
#       kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
#       verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
#       ca_file "#{ENV['KUBERNETES_CA_FILE']}"
#       skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
#       skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
#       skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
#       skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
#     </filter>
#   pods-fluent.conf: |-
#     <source>
#       @type tail
#       read_from_head true
#       tag kubernetes.*
#       path /var/log/containers/*.log
#       pos_file /var/log/fluentd-containers.log.pos
#       exclude_path ["/var/log/containers/fluent*"]
#       <parse>
#         @type kubernetes
#         @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
#         time_format %Y-%m-%dT%H:%M:%S.%NZ
#       </parse>
#     </source>

#     <filter kubernetes.**>
#       @type kubernetes_metadata
#       @id filter_kube_metadata
#       kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
#       verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
#       ca_file "#{ENV['KUBERNETES_CA_FILE']}"
#       skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
#       skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
#       skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
#       skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
#     </filter>
#   file-fluent.conf: |-
#     <match **>
#       @type file
#       path /tmp/file-test.log
#     </match>
#   elastic-fluent.conf: |-
#     <match **>
#       @type elasticsearch
#       host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch.logging'}"
#       port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
#       index_name fluentd-k8s
#       type_name fluentd
#     </match>

# Fluentd configurations with json parsing support:
